# -*- coding: utf-8 -*-
import os
import time
import logging
import pandas as pd
import numpy as np
import psycopg2
from psycopg2.extras import execute_values, RealDictCursor
from binance.client import Client
from datetime import datetime, timedelta, timezone
from decouple import config
from typing import List, Optional, Dict, Tuple
from threading import Thread
from flask import Flask
from scipy.signal import find_peaks
from sklearn.cluster import DBSCAN
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==============================================================================
# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© ÙˆÙ†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) ---
# ==============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('combined_analyzer.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('CombinedCryptoAnalyzer')

# ==============================================================================
# --- 2. ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ø«ÙˆØ§Ø¨Øª ---
# ==============================================================================
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    DB_URL: str = config('DATABASE_URL')
    logger.info("âœ… [Config] ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø¨Ù†Ø¬Ø§Ø­.")
except Exception as e:
    logger.critical(f"âŒ [Config] ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©. ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù .env: {e}")
    exit(1)

# --- Ø«ÙˆØ§Ø¨Øª Ø¹Ø§Ù…Ø© ---
RUN_INTERVAL_HOURS: int = 2  # Ø§Ù„ÙØ§ØµÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„ØªØ´ØºÙŠÙ„ (4 Ø³Ø§Ø¹Ø§Øª)
CRYPTO_LIST_FILENAME: str = 'crypto_list.txt'
MAX_WORKERS: int = 10 # Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
API_RETRY_ATTEMPTS: int = 3
API_RETRY_DELAY: int = 5

# --- Ø«ÙˆØ§Ø¨Øª Ø­Ø§Ø³Ø¨Ø© Ø¥ÙŠØ´ÙŠÙ…ÙˆÙƒÙˆ (Ichimoku) ---
ICHIMOKU_TIMEFRAME: str = '15m'
ICHIMOKU_DATA_LOOKBACK_DAYS: int = 30
ICHIMOKU_TENKAN_PERIOD: int = 9
ICHIMOKU_KIJUN_PERIOD: int = 26
ICHIMOKU_SENKOU_B_PERIOD: int = 52
ICHIMOKU_CHIKOU_SHIFT: int = -26
ICHIMOKU_SENKOU_SHIFT: int = 26

# --- Ø«ÙˆØ§Ø¨Øª Ù…Ø§Ø³Ø­ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© (S/R) ---
SR_DATA_FETCH_DAYS_1H: int = 30
SR_DATA_FETCH_DAYS_15M: int = 7
SR_DATA_FETCH_DAYS_5M: int = 3
SR_ATR_PERIOD: int = 14
SR_CLUSTER_EPS_PERCENT: float = 0.0015
SR_CONFLUENCE_ZONE_PERCENT: float = 0.002

# ==============================================================================
# --- 3. Ø¯ÙˆØ§Ù„ Ù…Ø´ØªØ±ÙƒØ© (Binance, Database, Utilities) ---
# ==============================================================================

def get_binance_client() -> Optional[Client]:
    """Initializes and returns the Binance client."""
    try:
        client = Client(API_KEY, API_SECRET)
        client.ping()
        logger.info("âœ… [Binance] ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Binance Ø¨Ù†Ø¬Ø§Ø­.")
        return client
    except Exception as e:
        logger.error(f"âŒ [Binance] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª: {e}")
        return None

def init_db() -> Optional[psycopg2.extensions.connection]:
    """Initializes and returns a new database connection."""
    try:
        conn = psycopg2.connect(DB_URL, cursor_factory=RealDictCursor)
        logger.info("âœ… [DB] ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ Ø¬Ø¯ÙŠØ¯ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
        return conn
    except Exception as e:
        logger.error(f"âŒ [DB] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        return None

def setup_database_tables(conn: psycopg2.extensions.connection):
    """Creates all required tables if they don't exist."""
    if not conn:
        logger.warning("[DB] Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ØªÙ… ØªØ®Ø·ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„.")
        return
    
    queries = [
        """
        CREATE TABLE IF NOT EXISTS ichimoku_features (
            id SERIAL PRIMARY KEY,
            symbol VARCHAR(20) NOT NULL,
            timestamp TIMESTAMPTZ NOT NULL,
            timeframe VARCHAR(10) NOT NULL,
            tenkan_sen FLOAT,
            kijun_sen FLOAT,
            senkou_span_a FLOAT,
            senkou_span_b FLOAT,
            chikou_span FLOAT,
            UNIQUE (symbol, timestamp, timeframe)
        );
        """,
        "CREATE INDEX IF NOT EXISTS idx_ichimoku_symbol_timestamp ON ichimoku_features (symbol, timestamp DESC);",
        """
        CREATE TABLE IF NOT EXISTS support_resistance_levels (
            id SERIAL PRIMARY KEY,
            symbol TEXT NOT NULL,
            level_price DOUBLE PRECISION NOT NULL,
            level_type TEXT NOT NULL,
            timeframe TEXT NOT NULL,
            strength NUMERIC NOT NULL,
            score NUMERIC DEFAULT 0,
            last_tested_at TIMESTAMP WITH TIME ZONE,
            details TEXT,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            UNIQUE (symbol, level_price, timeframe, level_type, details)
        );
        """
    ]
    try:
        with conn.cursor() as cur:
            for query in queries:
                cur.execute(query)
            # Check and add 'score' column if missing (for backward compatibility)
            cur.execute("SELECT 1 FROM information_schema.columns WHERE table_name='support_resistance_levels' AND column_name='score'")
            if not cur.fetchone():
                cur.execute("ALTER TABLE support_resistance_levels ADD COLUMN score NUMERIC DEFAULT 0;")
                logger.info("âœ… [DB] ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø¹Ù…ÙˆØ¯ 'score' Ø¥Ù„Ù‰ Ø¬Ø¯ÙˆÙ„ 'support_resistance_levels'.")

        conn.commit()
        logger.info("âœ… [DB] ØªÙ… ÙØ­Øµ/Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.error(f"âŒ [DB] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„: {e}")
        conn.rollback()

def get_validated_symbols(client: Client, filename: str) -> List[str]:
    """Reads a list of symbols from a file and validates them against Binance."""
    if not client:
        logger.error("âŒ [Symbol Validation] Binance client not available.")
        return []
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        if not os.path.exists(file_path):
            logger.error(f"âŒ Ù…Ù„Ù Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ {file_path}")
            return []
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_symbols = {line.strip().upper() for line in f if line.strip() and not line.startswith('#')}
        
        formatted_symbols = {f"{s}USDT" if not s.endswith('USDT') else s for s in raw_symbols}
        exchange_info = client.get_exchange_info()
        active_symbols = {s['symbol'] for s in exchange_info['symbols'] if s.get('quoteAsset') == 'USDT' and s.get('status') == 'TRADING'}
        
        validated_list = sorted(list(formatted_symbols.intersection(active_symbols)))
        logger.info(f"âœ… [Symbols] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(validated_list)} Ø¹Ù…Ù„Ø© ØµØ§Ù„Ø­Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")
        return validated_list
    except Exception as e:
        logger.error(f"âŒ [Symbol Validation] Ø®Ø·Ø£: {e}", exc_info=True)
        return []

def fetch_historical_data(client: Client, symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    """Fetches historical kline data from Binance with a retry mechanism."""
    if not client: return None
    for attempt in range(API_RETRY_ATTEMPTS):
        try:
            start_str = (datetime.now(timezone.utc) - timedelta(days=days)).strftime("%Y-%m-%d %H:%M:%S")
            klines = client.get_historical_klines(symbol, interval, start_str)
            if not klines:
                logger.warning(f"âš ï¸ [{symbol}] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ ÙØ±ÙŠÙ… {interval}.")
                return None
            
            df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'])
            numeric_cols = ['open', 'high', 'low', 'close', 'volume']
            df = df[numeric_cols + ['timestamp']]
            for col in numeric_cols:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)
            df.set_index('timestamp', inplace=True)
            return df.dropna()
        except Exception as e:
            logger.error(f"âŒ [{symbol}] Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}/{API_RETRY_ATTEMPTS}): {e}")
            if attempt < API_RETRY_ATTEMPTS - 1:
                time.sleep(API_RETRY_DELAY)
    logger.critical(f"âŒ [{symbol}] ÙØ´Ù„ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ {API_RETRY_ATTEMPTS} Ù…Ø­Ø§ÙˆÙ„Ø§Øª.")
    return None

# ==============================================================================
# --- 4. Ù‚Ø³Ù… Ø­Ø§Ø³Ø¨Ø© Ø¥ÙŠØ´ÙŠÙ…ÙˆÙƒÙˆ (Ichimoku) ---
# ==============================================================================

def calculate_ichimoku(df: pd.DataFrame) -> pd.DataFrame:
    """Calculates all Ichimoku Cloud components."""
    high, low, close = df['high'], df['low'], df['close']
    df['tenkan_sen'] = (high.rolling(window=ICHIMOKU_TENKAN_PERIOD).max() + low.rolling(window=ICHIMOKU_TENKAN_PERIOD).min()) / 2
    df['kijun_sen'] = (high.rolling(window=ICHIMOKU_KIJUN_PERIOD).max() + low.rolling(window=ICHIMOKU_KIJUN_PERIOD).min()) / 2
    df['senkou_span_a'] = ((df['tenkan_sen'] + df['kijun_sen']) / 2).shift(ICHIMOKU_SENKOU_SHIFT)
    df['senkou_span_b'] = ((high.rolling(window=ICHIMOKU_SENKOU_B_PERIOD).max() + low.rolling(window=ICHIMOKU_SENKOU_B_PERIOD).min()) / 2).shift(ICHIMOKU_SENKOU_SHIFT)
    df['chikou_span'] = close.shift(ICHIMOKU_CHIKOU_SHIFT)
    return df

def save_ichimoku_to_db(conn: psycopg2.extensions.connection, symbol: str, df_ichimoku: pd.DataFrame, timeframe: str):
    """Saves the calculated Ichimoku features to the database using ON CONFLICT."""
    if not conn or df_ichimoku.empty: return
    
    df_to_save = df_ichimoku[['tenkan_sen', 'kijun_sen', 'senkou_span_a', 'senkou_span_b', 'chikou_span']].copy()
    ichimoku_cols = df_to_save.columns.tolist()
    df_to_save.dropna(subset=ichimoku_cols, how='all', inplace=True)
    if df_to_save.empty: return

    df_to_save.reset_index(inplace=True)
    data_to_insert = [(symbol, row[0], timeframe) + tuple(row[1:]) for row in df_to_save[['timestamp'] + ichimoku_cols].to_numpy()]
    cols = ['symbol', 'timestamp', 'timeframe'] + ichimoku_cols
    update_cols = [f"{col} = EXCLUDED.{col}" for col in ichimoku_cols]
    query = f"INSERT INTO ichimoku_features ({', '.join(cols)}) VALUES %s ON CONFLICT (symbol, timestamp, timeframe) DO UPDATE SET {', '.join(update_cols)};"
    
    try:
        with conn.cursor() as cur:
            execute_values(cur, query, data_to_insert)
        conn.commit()
        logger.info(f"ğŸ’¾ [Ichimoku] ØªÙ… Ø­ÙØ¸/ØªØ­Ø¯ÙŠØ« {len(data_to_insert)} Ø³Ø¬Ù„ Ù„Ù€ {symbol}.")
    except Exception as e:
        logger.error(f"âŒ [DB-Ichimoku] Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª {symbol}: {e}")
        conn.rollback()

def run_ichimoku_analysis(client: Client, conn: psycopg2.extensions.connection, symbols: List[str]):
    """Main function for the Ichimoku calculation part."""
    logger.info("ğŸš€ [Ichimoku] Ø¨Ø¯Ø¡ Ø¯ÙˆØ±Ø© Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø¥ÙŠØ´ÙŠÙ…ÙˆÙƒÙˆ...")
    if not symbols:
        logger.warning("[Ichimoku] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù…Ù„Ø§Øª Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§.")
        return

    for symbol in symbols:
        logger.info(f"--- â³ [Ichimoku] Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {symbol} ---")
        try:
            df_ohlc = fetch_historical_data(client, symbol, ICHIMOKU_TIMEFRAME, ICHIMOKU_DATA_LOOKBACK_DAYS)
            if df_ohlc is None or df_ohlc.empty:
                logger.warning(f"[Ichimoku] Ù„Ù… ÙŠØªÙ… Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}. Ø³ÙŠØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ.")
                continue
            
            df_with_ichimoku = calculate_ichimoku(df_ohlc)
            save_ichimoku_to_db(conn, symbol, df_with_ichimoku, ICHIMOKU_TIMEFRAME)
        except Exception as e:
            logger.error(f"âŒ [Ichimoku] Ø®Ø·Ø£ Ø­Ø±Ø¬ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {symbol}: {e}", exc_info=True)
        time.sleep(1) # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
    logger.info("âœ… [Ichimoku] Ø§Ù†ØªÙ‡Øª Ø¯ÙˆØ±Ø© Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø¥ÙŠØ´ÙŠÙ…ÙˆÙƒÙˆ.")

# ==============================================================================
# --- 5. Ù‚Ø³Ù… Ù…Ø§Ø³Ø­ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© (S/R) ---
# ==============================================================================

def calculate_atr(df: pd.DataFrame, period: int) -> float:
    """Calculates Average True Range (ATR)."""
    if df.empty or len(df) < period: return 0
    high_low = df['high'] - df['low']
    high_close = np.abs(df['high'] - df['close'].shift())
    low_close = np.abs(df['low'] - df['close'].shift())
    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    atr = tr.ewm(alpha=1/period, adjust=False).mean()
    return atr.iloc[-1] if not atr.empty else 0

def find_price_action_levels(df: pd.DataFrame, atr_value: float) -> List[Dict]:
    """Finds support and resistance levels based on price action peaks."""
    lows, highs = df['low'].to_numpy(), df['high'].to_numpy()
    prominence = atr_value * 0.7 
    if prominence == 0: prominence = highs.mean() * 0.01

    low_peaks_indices, _ = find_peaks(-lows, prominence=prominence, width=3)
    high_peaks_indices, _ = find_peaks(highs, prominence=prominence, width=3)

    def cluster_levels(prices: np.ndarray, indices: np.ndarray, level_type: str) -> List[Dict]:
        if len(indices) < 2: return []
        points = prices[indices].reshape(-1, 1)
        eps_value = points.mean() * SR_CLUSTER_EPS_PERCENT
        if eps_value == 0: return []
        db = DBSCAN(eps=eps_value, min_samples=2).fit(points)
        clustered = []
        for label in set(db.labels_):
            if label != -1:
                mask = (db.labels_ == label)
                cluster_indices = indices[mask]
                clustered.append({
                    "level_price": float(prices[cluster_indices].mean()),
                    "level_type": level_type,
                    "strength": int(len(cluster_indices)),
                    "last_tested_at": df.index[cluster_indices[-1]].to_pydatetime()
                })
        return clustered
        
    return cluster_levels(lows, low_peaks_indices, 'support') + cluster_levels(highs, high_peaks_indices, 'resistance')

def calculate_fibonacci_levels(df: pd.DataFrame) -> List[Dict]:
    """Calculates Fibonacci retracement levels."""
    if df.empty: return []
    max_high, min_low = df['high'].max(), df['low'].min()
    diff = max_high - min_low
    if diff <= 0: return []
    
    fib_levels = []
    for ratio in [0.236, 0.382, 0.5, 0.618, 0.786]:
        details_s = f"Fib Support {ratio*100:.1f}%" + (" (Golden)" if ratio == 0.618 else "")
        fib_levels.append({"level_price": float(max_high - diff * ratio), "level_type": 'fib_support', "strength": 2, "details": details_s, "last_tested_at": None})
        details_r = f"Fib Resistance {ratio*100:.1f}%" + (" (Golden)" if ratio == 0.618 else "")
        fib_levels.append({"level_price": float(min_low + diff * ratio), "level_type": 'fib_resistance', "strength": 2, "details": details_r, "last_tested_at": None})
    return fib_levels

def find_confluence_zones(levels: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
    """Finds confluence zones where multiple levels cluster together."""
    if not levels: return [], []
    levels.sort(key=lambda x: x['level_price'])
    confluence_zones, used_indices = [], set()
    for i in range(len(levels)):
        if i in used_indices: continue
        current_zone_levels = [levels[i]]
        current_zone_indices = {i}
        for j in range(i + 1, len(levels)):
            if j in used_indices: continue
            price_i, price_j = levels[i]['level_price'], levels[j]['level_price']
            if price_i > 0 and (abs(price_j - price_i) / price_i) <= SR_CONFLUENCE_ZONE_PERCENT:
                current_zone_levels.append(levels[j])
                current_zone_indices.add(j)
        
        if len(current_zone_levels) > 1:
            used_indices.update(current_zone_indices)
            total_strength = sum(l['strength'] for l in current_zone_levels)
            if total_strength == 0: continue
            avg_price = sum(l['level_price'] * l['strength'] for l in current_zone_levels) / total_strength
            timeframes = sorted(list(set(str(l['timeframe']) for l in current_zone_levels)))
            details = sorted(list(set(l['level_type'] for l in current_zone_levels)))
            last_tested = max((l['last_tested_at'] for l in current_zone_levels if l['last_tested_at']), default=None)
            confluence_zones.append({
                "level_price": avg_price, "level_type": 'confluence', "strength": float(total_strength),
                "timeframe": ",".join(timeframes), "details": ",".join(details), "last_tested_at": last_tested
            })
            
    remaining_levels = [level for i, level in enumerate(levels) if i not in used_indices]
    return confluence_zones, remaining_levels

def calculate_level_score(level: Dict) -> int:
    """Calculates a score for a given level to rank its importance."""
    score = float(level.get('strength', 1)) * 10
    if level.get('level_type') == 'confluence': score *= 1.5
    if 'Golden' in level.get('details', ''): score += 25
    return int(score)

def analyze_single_symbol_sr(symbol: str, client: Client) -> List[Dict]:
    """Analyzes a single symbol for S/R levels across multiple timeframes."""
    logger.info(f"--- [S/R] Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ù„Ù€: {symbol} ---")
    raw_levels = []
    timeframes_config = {
        '1h':  {'days': SR_DATA_FETCH_DAYS_1H},
        '15m': {'days': SR_DATA_FETCH_DAYS_15M},
        '5m':  {'days': SR_DATA_FETCH_DAYS_5M}
    }

    for tf, config in timeframes_config.items():
        df = fetch_historical_data(client, symbol, tf, config['days'])
        if df is not None and not df.empty:
            atr = calculate_atr(df, SR_ATR_PERIOD)
            pa_levels = find_price_action_levels(df, atr)
            fib_levels = calculate_fibonacci_levels(df) if tf == '15m' else []
            
            for level in pa_levels + fib_levels:
                level['timeframe'] = tf
            raw_levels.extend(pa_levels + fib_levels)
        else:
            logger.warning(f"âš ï¸ [{symbol}-{tf}] ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª S/RØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ.")
    
    if not raw_levels: return []

    confluence, singles = find_confluence_zones(raw_levels)
    final_levels = confluence + singles
    for level in final_levels:
        level['symbol'] = symbol
        level['score'] = calculate_level_score(level)
        
    logger.info(f"--- âœ… [S/R] Ø§Ù†ØªÙ‡Ù‰ ØªØ­Ù„ÙŠÙ„ {symbol}ØŒ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(final_levels)} Ù…Ø³ØªÙˆÙ‰ Ù†Ù‡Ø§Ø¦ÙŠ.")
    return final_levels

def save_sr_levels_to_db(conn: psycopg2.extensions.connection, all_levels: List[Dict]):
    """Batch saves all found S/R levels to the database, deleting old ones first."""
    if not all_levels:
        logger.info("â„¹ï¸ [DB-S/R] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙˆÙŠØ§Øª Ù„Ø­ÙØ¸Ù‡Ø§.")
        return
    
    logger.info(f"â³ [DB-S/R] Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ {len(all_levels)} Ù…Ø³ØªÙˆÙ‰ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    try:
        with conn.cursor() as cur:
            symbols = list(set(level['symbol'] for level in all_levels))
            cur.execute("DELETE FROM support_resistance_levels WHERE symbol = ANY(%s);", (symbols,))
            logger.info(f"[DB-S/R] ØªÙ… Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„Ù€ {len(symbols)} Ø¹Ù…Ù„Ø©.")
            
            insert_query = """
                INSERT INTO support_resistance_levels (symbol, level_price, level_type, timeframe, strength, score, last_tested_at, details) 
                VALUES %s ON CONFLICT (symbol, level_price, timeframe, level_type, details) DO NOTHING;
            """
            values = [(l['symbol'], l['level_price'], l['level_type'], l['timeframe'], l['strength'], l['score'], l.get('last_tested_at'), l.get('details')) for l in all_levels]
            execute_values(cur, insert_query, values)
        conn.commit()
        logger.info(f"âœ… [DB-S/R] ØªÙ… Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.error(f"âŒ [DB-S/R] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ø¬Ù…Ø¹: {e}", exc_info=True)
        conn.rollback()

def run_sr_analysis(client: Client, conn: psycopg2.extensions.connection, symbols: List[str]):
    """Main function for the S/R analysis part."""
    logger.info("ğŸš€ [S/R] Ø¨Ø¯Ø¡ Ø¯ÙˆØ±Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©...")
    if not symbols:
        logger.warning("[S/R] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù…Ù„Ø§Øª Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§.")
        return

    all_final_levels = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_symbol = {executor.submit(analyze_single_symbol_sr, symbol, client): symbol for symbol in symbols}
        for i, future in enumerate(as_completed(future_to_symbol)):
            symbol = future_to_symbol[future]
            try:
                symbol_levels = future.result()
                if symbol_levels: all_final_levels.extend(symbol_levels)
                logger.info(f"ğŸ”„ [S/R] ({i+1}/{len(symbols)}) ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†ØªØ§Ø¦Ø¬ {symbol}.")
            except Exception as e:
                logger.error(f"âŒ [S/R] Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ {symbol}: {e}", exc_info=True)

    if all_final_levels:
        all_final_levels.sort(key=lambda x: x.get('score', 0), reverse=True)
        save_sr_levels_to_db(conn, all_final_levels)
    
    logger.info("âœ… [S/R] Ø§Ù†ØªÙ‡Øª Ø¯ÙˆØ±Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©.")

# ==============================================================================
# --- 6. Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„Ø© ÙˆØ®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ ---
# ==============================================================================

def main_analysis_job():
    """
    The main scheduled job that runs both Ichimoku and S/R analysis in a loop.
    """
    while True:
        logger.info(f" ciclo de anÃ¡lisis combinado... PrÃ³xima ejecuciÃ³n en {RUN_INTERVAL_HOURS} horas.")
        
        client = None
        conn = None
        
        try:
            # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© ÙƒÙ„ Ø¯ÙˆØ±Ø©
            client = get_binance_client()
            conn = init_db()

            if not client or not conn:
                logger.critical("âŒ ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª. Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ±Ø©.")
            else:
                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
                setup_database_tables(conn)
                
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©
                symbols_to_process = get_validated_symbols(client, CRYPTO_LIST_FILENAME)
                
                if not symbols_to_process:
                    logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù…Ù„Ø§Øª ØµØ§Ù„Ø­Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ±Ø©.")
                else:
                    # --- ØªØ´ØºÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø¥ÙŠØ´ÙŠÙ…ÙˆÙƒÙˆ ---
                    run_ichimoku_analysis(client, conn, symbols_to_process)
                    
                    # --- ØªØ´ØºÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© ---
                    run_sr_analysis(client, conn, symbols_to_process)

        except Exception as e:
            logger.critical(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø­Ù„Ù‚Ø© Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {e}", exc_info=True)
        finally:
            # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø¯ÙˆØ±Ø©
            if conn:
                conn.close()
                logger.info("âœ… [DB] ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ±Ø©.")
        
        logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ø¯ÙˆØ±Ø©. Ø³ÙŠØªÙ… Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ø¯Ø© {RUN_INTERVAL_HOURS} Ø³Ø§Ø¹Ø§Øª.")
        time.sleep(RUN_INTERVAL_HOURS * 60 * 60)

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ (Flask) ---
app = Flask(__name__)
@app.route('/')
def health_check():
    """Health check endpoint for the hosting platform."""
    return "âœ… Combined Crypto Analyzer (Ichimoku + S/R) service is running.", 200

# --- Ù†Ù‚Ø·Ø© Ø§Ù†Ø·Ù„Ø§Ù‚ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ---
if __name__ == "__main__":
    # ØªØ´ØºÙŠÙ„ Ù…Ù‡Ù…Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„
    analysis_thread = Thread(target=main_analysis_job, daemon=True)
    analysis_thread.start()
    
    # ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ ÙÙŠ Ø§Ù„Ø®ÙŠØ· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    port = int(os.environ.get("PORT", 10000))
    logger.info(f"ğŸŒ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… ÙØ­Øµ Ø§Ù„Ø­Ø§Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ° {port}")
    # Ø§Ø³ØªØ®Ø¯Ù… 'waitress' Ø£Ùˆ 'gunicorn' ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† app.run()
    app.run(host='0.0.0.0', port=port)
